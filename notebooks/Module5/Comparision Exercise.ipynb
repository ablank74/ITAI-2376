{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Assignment - Comparison Exercise - Analyzing Deep Learning Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will try using both TensorFlow and PyTorch to train a model on a bird species dataset.  We will look at runtime information as well as CPU/GPU/Memory load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /usr/local/lib/python3.8/dist-packages (1.6.6)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kaggle) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: python-slugify in /usr/local/lib/python3.8/dist-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.8/dist-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from kaggle) (4.66.2)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.8/dist-packages (from kaggle) (2.2.1)\n",
      "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.8/dist-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->kaggle) (3.6)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "\n",
    "# Download the bird identification dataset from Kaggle https://www.kaggle.com/datasets/gpiosenka/100-bird-species\n",
    "dataset_name = 'gpiosenka/100-bird-species'\n",
    "path_to_download = 'birds'  # Specify your download path here\n",
    "\n",
    "api.dataset_download_files(dataset_name, path=path_to_download, unzip=True)\n",
    "\n",
    "print(f'Dataset downloaded and extracted to: {path_to_download}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with the Kaggle 525 Bird Species dataset using TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Tensor Flow\n",
    "import tensorflow\n",
    "# Import Keras\n",
    "from tensorflow import keras\n",
    "# Import Numpy\n",
    "import numpy as np\n",
    "# Import time for measuring the time taken to build the model\n",
    "import time\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# import the dataset\n",
    "train_data = keras.utils.image_dataset_from_directory('birds/train', image_size=(224, 224), batch_size=32)\n",
    "test_data = keras.utils.image_dataset_from_directory('birds/test', image_size=(224, 224), batch_size=32)\n",
    "validation_data = keras.utils.image_dataset_from_directory('birds/valid', image_size=(224, 224), batch_size=32)\n",
    "# Load the class names from csv file\n",
    "class_names = np.loadtxt('birds/birds.csv', delimiter=',', usecols=(1,), dtype=str, skiprows=1)\n",
    "# Print the class names\n",
    "print(class_names)\n",
    "# Print the shape of the training data\n",
    "print(train_data)\n",
    "# Print the shape of the test data\n",
    "print(test_data)\n",
    "# Print the shape of the validation data\n",
    "print(validation_data)\n",
    "\n",
    "\n",
    "# Example of normalization for image data\n",
    "train_data = train_data.map(lambda x, y: (x / 255.0, y))\n",
    "test_data = test_data.map(lambda x, y: (x / 255.0, y))\n",
    "validation_data = validation_data.map(lambda x, y: (x / 255.0, y))\n",
    "\n",
    "# Create a CNN model\n",
    "tf_model = keras.Sequential([\n",
    "    keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    keras.layers.MaxPooling2D(),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(len(class_names), activation='softmax')\n",
    "])\n",
    "# Compile the model\n",
    "tf_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "tf_model.fit(train_data, epochs=5, validation_data=validation_data)\n",
    "\n",
    "# Evaluate the model\n",
    "tf_model.evaluate(test_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = tf_model.predict(test_data)\n",
    "# Print the predictionsf\n",
    "print(predictions)\n",
    "\n",
    "stop_time = time.perf_counter()\n",
    "duration_seconds = stop_time - start_time\n",
    "\n",
    "# Convert seconds to minutes and seconds for easier reading\n",
    "minutes = duration_seconds // 60\n",
    "seconds = duration_seconds % 60\n",
    "\n",
    "print(f\"The code execution took {int(minutes)} minutes and {seconds} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with the 525 Bird Species dataset using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using:\", device)\n",
    "\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.ImageFolder('birds/train', transform=transform)\n",
    "test_dataset = datasets.ImageFolder('birds/test', transform=transform)\n",
    "validation_dataset = datasets.ImageFolder('birds/valid', transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load the class names from csv file\n",
    "class_names = np.loadtxt('birds/birds.csv', delimiter=',', usecols=(1,), dtype=str, skiprows=1)\n",
    "# Print the class names\n",
    "print(class_names)\n",
    "# Print the shape of the training data\n",
    "print(train_dataset)\n",
    "# Print the shape of the test data\n",
    "print(test_dataset)\n",
    "# Print the shape of the validation data\n",
    "print(validation_dataset)\n",
    "\n",
    "\n",
    "# Creating a temporary model to determine input size for first linear layer\n",
    "temp_model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 32, 3, 1),\n",
    "    torch.nn.MaxPool2d(2),\n",
    "    torch.nn.Flatten()\n",
    ")\n",
    "\n",
    "x, _ = next(iter(train_loader))\n",
    "x = temp_model(x)\n",
    "print('Temporary Model Shape',x.shape)\n",
    "\n",
    "\n",
    "# Create a CNN model with PyTorch\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(3, 32, 3, 1),\n",
    "    torch.nn.MaxPool2d(2),\n",
    "    torch.nn.Flatten(),\n",
    "    torch.nn.Linear(394272, 128),\n",
    "    torch.nn.Linear(128, len(class_names))\n",
    ")\n",
    "\n",
    "# move the model to the GPU\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):\n",
    "    progress_bar = tqdm.tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for x, y in train_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.set_description(f'Epoch {epoch + 1}/{5}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    progress_bar = tqdm.tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for x, y in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        y_pred = model(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        progress_bar.set_description(f'Accuracy: {correct / total}')\n",
    "\n",
    "print(f'Accuracy: {correct / total}')\n",
    "\n",
    "\n",
    "# Timing the model building\n",
    "stop_time = time.perf_counter()\n",
    "duration_seconds = stop_time - start_time\n",
    "\n",
    "# Convert seconds to minutes and seconds for easier reading\n",
    "minutes = duration_seconds // 60\n",
    "seconds = duration_seconds % 60\n",
    "\n",
    "print(f\"The code execution took {int(minutes)} minutes and {seconds} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "From here we see that the TensorFlow option ran quicker.  The loss ration for the PyTorch option is lower though.  These loss ratios could be improved with more work eithe preprocessing the data, or by looking at changing the way the model is built, possibly adding some dropout to reduce overfitting.\n",
    "\n",
    "CPU utilization was fairly low on both methods since a GPU was available, and the GPU utilization was near 100% the duration of the training.  Memroy utilization seemed to be slightly lower using PyTorch, but the difference was too small to be sure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
